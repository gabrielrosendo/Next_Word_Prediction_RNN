TODO:
- Model still not learning pattern/learning very slowly
    - Increase subset size: 
    change line data = data[:1000000] -> Tried this but if dataset is too large code does not run
                        -> Also makes epochs really slow
                -> Try to make subset larger without those issues to better accuracy
    - Change model parameters
- Add garbage collection to make it run faster?
- Check how data was prepared and proccessed -> Might be wrong
- Experiment with larger LSTM/GRU units and bidirectional layers.
- add regularization and early stopping.




Requirements for assignment:

Set the number of words as a parameter.
Select a subset of the data to be used as a training set.
Decide on a training method, for example:
Use words 1 through n words as features and the n+1 word as the label.

Use words 2 through n+1 as features and then the n+2 word as the label.

Use words 3 through n+2 as features and then the n+3 word as the label.

Etc.

Note: The performance of the RNN is proportional to the amount of data.

Build a long short-term memory (LSTM) model with embedding and dense layers (use the Keras Sequential API):

An Embedding, which maps each input word to a 100-dimensional vector. The embedding can use pretrained weights (more in a second), which we supply in the weights parameter; trainable can be set False if we don't want to update the embeddings.
A Masking layer, to mask any words that do not have a pretrained embedding, which will be represented as all zeros. This layer should not be used when training the embeddings.
The heart of the network: a layer of LSTM cells with dropout to prevent overfitting. Since we are only using one LSTM layer, it does not return the sequences; for using two or more layers, make sure to return sequences.
A fully connected Dense layer with relu. This adds additional representational capacity to the network.
A Dropout layer, to prevent overfitting to the training data.
A Dense fully connected output layer, this produces a probability for every word in the vocab using softmax.
Compile the model with the Adam optimizer.

Load in the pretrained embeddings; choose embeddings from the GloVe algorithm, which has been trained on Wikipedia texts. Refer to "GloVe: Global Vectors for Word Representation," located in the topic Resources. Assign a 100-dimensional vector to each word in the vocab. If the word has no pretrained embedding, then this vector will be all zeros.

Explore the embeddings using cosine similarity between two vectors.

Train the model to predict the next word in the sequence using:

Model Checkpoint: saves the best model (as measured by validation loss) on disk for using best model.
Early Stopping: halts training when validation loss is no longer decreasing.
Note: The training may take a considerable amount of time, possibly a few hours.

Make predictions by passing in a starting sequence (i.e., an incomplete sentence) and complete it.

Summarize the overall functioning of the RNN and its accuracy.

APA style is not required, but solid academic writing is expected.

This assignment uses a rubric. Please review the rubric prior to beginning the assignment to become familiar with the expectations for successful completion.

You are not required to submit this assignment to LopesWrite.   